{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 単語と文章のベクトル化\n",
    "\n",
    "自然言語のままで各種分析を行う際は、単語と文章のベクトル化が必要になります。\n",
    "\n",
    "単語のベクトル化：word2vec\n",
    "\n",
    "文章のベクトル化：doc2vec\n",
    "\n",
    "\n",
    "# 様々な学習済みモデル\n",
    "\n",
    "基本的に、word2vecは、著名な学習済みデータセットを使用することが多いです。\n",
    "\n",
    "doc2vecは、案件ごとに学習することが多いです。\n",
    "\n",
    "http://blog.hassaku-labs.com/post/pretrained-word2vec/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec（既存モデルの使用）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 読み込み\n",
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format('./data/entity_vector.model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mio\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.09032071, -0.05341686,  0.28220174,  0.29330444,  0.05328318,\n",
       "        0.14241028, -0.24687128,  0.18441112,  0.02650695,  0.0828563 ,\n",
       "       -0.426362  , -0.02014246,  0.02603034, -0.03726895,  0.19294073,\n",
       "       -0.12511627,  0.5407648 ,  0.05329642, -0.114064  , -0.08794833,\n",
       "        0.48768535,  0.16046745,  0.0785282 , -0.49404824,  0.11478704,\n",
       "       -0.32564625, -0.55675274, -0.30036446, -0.20920439,  0.06128695,\n",
       "       -0.05599485, -0.08785352, -0.17117216,  0.28584853,  0.01225359,\n",
       "        0.15091456, -0.07321789, -0.07828152,  0.16382505,  0.38058066,\n",
       "        0.19476666,  0.06204469, -0.27429718,  0.04294476, -0.05257293,\n",
       "        0.05322105, -0.15351981, -0.6151462 ,  0.16540758,  0.36411384,\n",
       "        0.03025797, -0.31749636,  0.2605783 , -0.05329959,  0.0940825 ,\n",
       "       -0.1286651 , -0.19278173, -0.3219691 ,  0.10385962,  0.25326288,\n",
       "       -0.00975231, -0.09564543, -0.3629564 , -0.20086282, -0.12936008,\n",
       "        0.43610513, -0.36885667,  0.295815  , -0.12997843,  0.4314916 ,\n",
       "       -0.14947411,  0.06331145,  0.01181818,  0.6452983 , -0.06020614,\n",
       "        0.05043695, -0.04708637,  0.01179598, -0.36667073, -0.17686577,\n",
       "        0.17143977,  0.3592317 ,  0.3453721 , -0.11092807,  0.03805619,\n",
       "        0.11667155, -0.21518715, -0.19824842,  0.7911886 , -0.28698277,\n",
       "        0.22000696,  0.6519969 ,  0.08775561, -0.03462984, -0.06396964,\n",
       "        0.10759465, -0.27445945,  0.12017197,  0.2182324 ,  0.02001271,\n",
       "       -0.2749907 ,  0.1472141 , -0.15745465, -0.36727196,  0.03987442,\n",
       "       -0.15021245, -0.12411412,  0.47442573, -0.27764642,  0.18471095,\n",
       "       -0.19148739,  0.4019693 ,  0.04580771, -0.01343833,  0.05986825,\n",
       "       -0.0702235 , -0.3262036 ,  0.21441376, -0.05585141,  0.4068481 ,\n",
       "       -0.20326735,  0.159737  ,  0.22966705, -0.2549111 , -0.38730478,\n",
       "       -0.2007898 , -0.12139512, -0.13236366, -0.1494875 , -0.8284076 ,\n",
       "        0.14567637, -0.14218427, -0.20600604, -0.02710883, -0.7757116 ,\n",
       "       -0.23637761, -0.28013197,  0.03547173,  0.08473831, -0.24143983,\n",
       "        0.5714134 , -0.20404778,  0.2857376 , -0.12174043, -0.05783201,\n",
       "        0.16254428, -0.11809638,  0.45789692,  0.567776  ,  0.1777195 ,\n",
       "       -0.4642111 ,  0.12196863, -0.25125453, -0.02837646, -0.11756613,\n",
       "       -0.27768514, -0.06383181, -0.13533409, -0.09328051,  0.01945806,\n",
       "       -0.04140085, -0.13909361,  0.08832241, -0.5650386 , -0.13418637,\n",
       "        0.01655536, -0.27362147, -0.12080104,  0.20296168,  0.02414615,\n",
       "        0.02504183, -0.20633349,  0.03708602,  0.26493123, -0.03714316,\n",
       "        0.23291802,  0.1900764 ,  0.5301852 ,  0.3596682 ,  0.06676093,\n",
       "        0.18211949,  0.01261326, -0.01917026,  0.00164132, -0.08637996,\n",
       "       -0.03631896,  0.6470705 ,  0.0400873 , -0.08204225,  0.15705019,\n",
       "       -0.06294098, -0.09003455,  0.09688611,  0.2371008 , -0.04234828,\n",
       "        0.12725645,  0.11786682, -0.5076251 ,  0.3587788 , -0.19403741],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 単語のベクトル化\n",
    "model.wv[\"イチロー\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[前田敦子]', 0.8524860143661499)\n",
      "('[松井玲奈]', 0.8497596979141235)\n",
      "('[柏木由紀]', 0.8481603860855103)\n",
      "('[大島優子]', 0.8426693677902222)\n",
      "('[渡辺麻友]', 0.836478590965271)\n",
      "('[高橋みなみ]', 0.8302541971206665)\n",
      "('[山本彩]', 0.8275360465049744)\n",
      "('[HKT48]', 0.8238900899887085)\n",
      "('[小嶋陽菜]', 0.8218119144439697)\n",
      "('[島崎遥香]', 0.8172259330749512)\n"
     ]
    }
   ],
   "source": [
    "# 類似単語の抽出\n",
    "results = model.most_similar(u\"[指原莉乃]\")\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ドレナージ', 0.4027651846408844)\n",
      "('[分娩]', 0.3961508870124817)\n",
      "('[座位]', 0.37961751222610474)\n",
      "('[出産]', 0.37935060262680054)\n",
      "('[生殖]', 0.3761950731277466)\n",
      "('望ましい', 0.37324926257133484)\n",
      "('[子育て]', 0.372659832239151)\n",
      "('[歯科医師]', 0.3716592788696289)\n",
      "('[歯学者]', 0.36931663751602173)\n",
      "('[公認心理師]', 0.36690229177474976)\n"
     ]
    }
   ],
   "source": [
    "# 減算\n",
    "results = model.most_similar(positive=[u'[妻]'],negative=[u'[愛人]'])\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[精霊]', 0.8402377367019653)\n",
      "('[魔女]', 0.8217898607254028)\n",
      "('[悪魔]', 0.8189244270324707)\n",
      "('悪霊', 0.8049401044845581)\n",
      "('創造主', 0.7825872898101807)\n",
      "('[吸血鬼]', 0.7805193662643433)\n",
      "('[悪霊]', 0.779639482498169)\n",
      "('精霊', 0.7730634212493896)\n",
      "('魔物', 0.771123468875885)\n",
      "('[エルフ]', 0.7656500339508057)\n"
     ]
    }
   ],
   "source": [
    "# 加算\n",
    "results = model.most_similar(positive=[u'[妖精]',u'[悪]'])\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('[ロナウド]', 0.6781752109527588)\n",
      "('[ジネディーヌ・ジダン]', 0.6686528325080872)\n",
      "('[中田英寿]', 0.6643422842025757)\n",
      "('[ジーコ]', 0.6631736755371094)\n",
      "('[ロベルト・バッジョ]', 0.6589246988296509)\n",
      "('[ロナウジーニョ]', 0.6574504971504211)\n",
      "('[ディエゴ・マラドーナ]', 0.6535028219223022)\n",
      "('ジダン', 0.6474759578704834)\n",
      "('[デビッド・ベッカム]', 0.6473382115364075)\n",
      "('[三浦知良]', 0.6451823711395264)\n"
     ]
    }
   ],
   "source": [
    "# 加算減算\n",
    "results = model.most_similar(positive=[u'[イチロー]',u'[サッカー]'],negative=[u'[野球]'])\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# doc2vec（自己学習）\n",
    "\n",
    "使用するデータは、青空文庫にある小説です。\n",
    "\n",
    "https://www.aozora.gr.jp/cards/000148/files/794_ruby_4237.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "一\n",
      "　うとうととして目がさめると女はいつのまにか、隣のじいさんと話を始めている。このじいさんはたしかに前の前の駅から乗ったいなか者である。発車まぎわに頓狂な声を出して駆け込んで来て、いきなり肌をぬい\n",
      "171924\n"
     ]
    }
   ],
   "source": [
    "# データの読み込み\n",
    "with open('./data/sanshiro.txt', encoding='sjis') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# ファイル整形\n",
    "import re\n",
    "text = re.split('\\-{5,}',text)[2]\n",
    "text = re.split('底本：',text)[0]\n",
    "text = text.replace('|', '')\n",
    "text = re.sub('《.+?》', '', text)\n",
    "text = re.sub('［＃.+?］', '',text)\n",
    "text = re.sub('\\n\\n', '\\n', text) \n",
    "text = re.sub('\\r', '', text)\n",
    "\n",
    "print(text[:100])\n",
    "\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下記のようなリストに変換する\n",
    "# [\n",
    "#  [単語1,単語2,....]\n",
    "#  [単語1,単語2,....]\n",
    "#  [単語1,単語2,....]\n",
    "# ]\n",
    "\n",
    "# ライブラリの読み込み\n",
    "from janome.tokenizer import Tokenizer\n",
    "def extract_data(text):\n",
    "    '''文章を渡したら、動詞と名詞のリストを返す\n",
    "    引数：\n",
    "    　text：一文\n",
    "    返り値：\n",
    "    　data：動詞と名詞のリスト\n",
    "    処理概要：\n",
    "    　1つの文章を受け取り、その中に含まれている名詞と動詞のリストを返す\n",
    "    '''\n",
    "    # インスタンス化\n",
    "    t = Tokenizer()\n",
    "    # 動詞と名詞のリスト\n",
    "    data=[]\n",
    "    # 形態素解析\n",
    "    res = t.tokenize(text)\n",
    "    for i in res:\n",
    "        data.append(i.surface)\n",
    "    return data\n",
    "\n",
    "# テキストを「。」で区切る\n",
    "text_list = text.split('。')\n",
    "\n",
    "# それぞれの文章を単語リストに変換(処理に数分かかります)\n",
    "word_list = [extract_data(text) for text in text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5904\n",
      "['一', '\\n', '\\u3000', 'うとうと', 'と', 'し', 'て', '目', 'が', 'さめる', 'と', '女', 'は', 'いつのまにか', '、', '隣', 'の', 'じいさん', 'と', '話', 'を', '始め', 'て', 'いる']\n"
     ]
    }
   ],
   "source": [
    "# 形式の確認\n",
    "print(len(word_list))\n",
    "\n",
    "print(word_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリ読み込み\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習用データに変換\n",
    "training_docs = []\n",
    "\n",
    "for i in word_list:\n",
    "    training_docs.append(\n",
    "        TaggedDocument(words=i,tags=[str(i)])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの学習\n",
    "model = Doc2Vec(documents=training_docs, dm = 1, size=30, window=8, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================\n",
      "文書A＝ 一\n",
      "　うとうととして目がさめると女はいつのまにか、隣のじいさんと話を始めている\n",
      "\n",
      "文書B＝ ただ腹の中で、これしきの女の言う事を、明瞭に批評しえないのは、男児としてふがいないことだと、いたく赤面した\n",
      "\n",
      "==========================\n",
      "文書A＝ このじいさんはたしかに前の前の駅から乗ったいなか者である\n",
      "\n",
      "文書B＝ 人は二十日足らずの目のさきに春を控えた\n",
      "\n",
      "==========================\n",
      "文書A＝ 発車まぎわに頓狂な声を出して駆け込んで来て、いきなり肌をぬいだと思ったら背中にお灸のあとがいっぱいあったので、三四郎の記憶に残っている\n",
      "\n",
      "文書B＝ 文芸時評の売れ高の少ないのは当人の自白したとおりであるのに、麗々しく彼のいわゆる大論文を掲げて得意がるのは、虚栄心の満足以外になんのためになるだろうと疑っていたが、これでみると活版の勢力はやはりたいしたものである\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 試しに類似文書抽出\n",
    "for i in range(3):\n",
    "    print('==========================')\n",
    "    print(\"文書A＝\",''.join(word_list[i]))\n",
    "    print()\n",
    "    print(\"文書B＝\",model.docvecs.most_similar(i)[0][0].replace('\"','').replace('[','').replace(']','').replace(\"'\",\"\").replace(',','').replace(' ',''))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文書A＝ 一\n",
      "　うとうととして目がさめると女はいつのまにか、隣のじいさんと話を始めている\n",
      "\n",
      "文書B＝ うしろから女がついて来る\n",
      "\n",
      "類似度 0.47301188\n"
     ]
    }
   ],
   "source": [
    "print(\"文書A＝\",''.join(word_list[0]))\n",
    "print()\n",
    "print(\"文書B＝\",''.join(word_list[100]))\n",
    "print()\n",
    "print(\"類似度\",model.docvecs.similarity(0,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実際の使い方\n",
    "\n",
    "上記で見た通り、似た意味を持つ単語や「単語の意味」の加算減算が出来ます。\n",
    "\n",
    "ベクトル化出来れば、NNなどの各種機械学習アルゴリズムに使用でき、目的によって柔軟な分析が可能になります。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
