{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF\n",
    "\n",
    "文書中に含まれる単語の重要度を評価する手法の1つ\n",
    "\n",
    "**tf：各文書においてその単語がどのくらい出現したのか**\n",
    "\n",
    "tf　=　文書Aにおける単語Xの出現頻度文書　/　文書Aにおける全単語の出現頻度の和\n",
    "\n",
    "**idf：「逆文書頻度」と呼ばれており、単語が「レア」なら高い値を、「色々な文書によく出現する単語」なら低い値を示す**\n",
    "\n",
    "idf　=　log(　全文書数　/　単語Xを含む文書数　)\n",
    "\n",
    "**tfidf：「その単語がよく出現するほど」「その単語がレアなほど」大きい値を示す**\n",
    "\n",
    "tfidf　=　tf　×　idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要ライブラリの読み込み\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文章群\n",
    "text_list = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>is</th>\n",
       "      <th>one</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.469791</td>\n",
       "      <td>0.580286</td>\n",
       "      <td>0.384085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.384085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.384085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.687624</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.281089</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538648</td>\n",
       "      <td>0.281089</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.281089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.511849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.267104</td>\n",
       "      <td>0.511849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.267104</td>\n",
       "      <td>0.511849</td>\n",
       "      <td>0.267104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.469791</td>\n",
       "      <td>0.580286</td>\n",
       "      <td>0.384085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.384085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.384085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        and  document     first        is       one    second       the  \\\n",
       "0  0.000000  0.469791  0.580286  0.384085  0.000000  0.000000  0.384085   \n",
       "1  0.000000  0.687624  0.000000  0.281089  0.000000  0.538648  0.281089   \n",
       "2  0.511849  0.000000  0.000000  0.267104  0.511849  0.000000  0.267104   \n",
       "3  0.000000  0.469791  0.580286  0.384085  0.000000  0.000000  0.384085   \n",
       "\n",
       "      third      this  \n",
       "0  0.000000  0.384085  \n",
       "1  0.000000  0.281089  \n",
       "2  0.511849  0.267104  \n",
       "3  0.000000  0.384085  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b')\n",
    "tfidf = tfidf_vectorizer.fit_transform(text_list)\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    data=tfidf.toarray(),\n",
    "    columns=tfidf_vectorizer.get_feature_names()\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDFコサイン類似度推定法\n",
    "### 二つの文章がどの程度に通った文章か判定する\n",
    "\n",
    "二つの単語ベクトルの内積をとる\n",
    "\n",
    "同じ方向を向いていれば（似通った内容）なら、1\n",
    "\n",
    "逆の方向を向いていれば（似ていない内容）なら、-1\n",
    "\n",
    "となる内積の性質を利用する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.64692568 0.30777187 1.        ]\n",
      " [0.64692568 1.         0.22523955 0.64692568]\n",
      " [0.30777187 0.22523955 1.         0.30777187]\n",
      " [1.         0.64692568 0.30777187 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "tfidf_array = tfidf.toarray()\n",
    "cosine_similarity_ = cosine_similarity(tfidf_array,tfidf_array)\n",
    "print(cosine_similarity_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実践編"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込み\n",
    "with open('./data/sanshiro.txt', encoding='sjis') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# テキスト整形\n",
    "import re\n",
    "text = re.split('\\-{5,}',text)[2]\n",
    "text = re.split('底本：',text)[0]\n",
    "text = text.replace('|', '')\n",
    "text = re.sub('《.+?》', '', text)\n",
    "text = re.sub('［＃.+?］', '',text)\n",
    "text = re.sub('\\n\\n', '\\n', text) \n",
    "text = re.sub('\\r', '', text)\n",
    "text = re.sub('　', '', text)\n",
    "text = re.sub(' ', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テキストを「。」で区切る\n",
    "text_list = text.split('。')\n",
    "\n",
    "from janome.tokenizer import Tokenizer\n",
    "def get_wakati(text):\n",
    "    '''わかち書きされた文書を返す\n",
    "    引数：\n",
    "    　text：1文\n",
    "    返り値：\n",
    "    　wakati_text：わかち書きされた文書\n",
    "    '''\n",
    "    # インスタンス化\n",
    "    t = Tokenizer()\n",
    "    # 動詞と名詞のリスト\n",
    "    wakati_text=''\n",
    "    # 形態素解析\n",
    "    res = t.tokenize(text)\n",
    "    for i in res:\n",
    "        wakati_text = wakati_text+i.surface+' '\n",
    "    return wakati_text\n",
    "\n",
    "wakati_text_list = [get_wakati(text) for text in text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF \n",
    "tfidf_vectorizer = TfidfVectorizer(token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b')\n",
    "tfidf = tfidf_vectorizer.fit_transform(wakati_text_list)\n",
    "data=tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================\n",
      "\n",
      "一\n",
      "　うとうととして目がさめると女はいつのまにか、隣のじいさんと話を始めている\n",
      "うとうと うとうと\n",
      "=================================\n",
      "このじいさんはたしかに前の前の駅から乗ったいなか者である\n",
      "いなか者 じいさん\n",
      "=================================\n",
      "発車まぎわに頓狂な声を出して駆け込んで来て、いきなり肌をぬいだと思ったら背中にお灸のあとがいっぱいあったので、三四郎の記憶に残っている\n",
      "ぬい ぬい\n",
      "=================================\n",
      "じいさんが汗をふいて、肌を入れて、女の隣に腰をかけたまでよく注意して見ていたくらいである\n",
      "ふい じいさん\n",
      "=================================\n",
      "\n",
      "　女とは京都からの相乗りである\n",
      "相乗り 京都\n",
      "=================================\n",
      "乗った時から三四郎の目についた\n",
      "乗っ つい\n",
      "=================================\n",
      "第一色が黒い\n",
      "一色 黒い\n",
      "=================================\n",
      "三四郎は九州から山陽線に移って、だんだん京大阪へ近づいて来るうちに、女の色が次第に白くなるのでいつのまにか故郷を遠のくような哀れを感じていた\n",
      "大阪 大阪\n",
      "=================================\n",
      "それでこの女が車室にはいって来た時は、なんとなく異性の味方を得た心持ちがした\n",
      "味方 味方\n",
      "=================================\n",
      "この女の色はじっさい九州色であった\n",
      "九州 じっさい\n",
      "=================================\n",
      "\n",
      "　三輪田のお光さんと同じ色である\n",
      "三輪田 同じ\n"
     ]
    }
   ],
   "source": [
    "# 各文書ごと、その文書内での最も重要な単語を抽出する\n",
    "for index,i in enumerate(data):\n",
    "    print(\"=================================\")\n",
    "    print(text_list[index])\n",
    "    # 最も重要度の高い単語のインデックス取得\n",
    "    best1_word_index = list(i).index(max(i))\n",
    "    # 最も重要度の高い単語の取得\n",
    "    best1_word=tfidf_vectorizer.get_feature_names()[best1_word_index]\n",
    "    # 最も重要度の高い単語のインデックス取得(2番目)\n",
    "    best2_word_index = list(i).index(sorted(i)[-2])\n",
    "    # 最も重要度の高い単語の取得(2番目)\n",
    "    best2_word=tfidf_vectorizer.get_feature_names()[best2_word_index]\n",
    "    print(best1_word,best2_word)\n",
    "    \n",
    "    if index==10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 実際の使い方\n",
    "\n",
    "重要な単語がわかれば、文書が何を表す文書かということも推定できます。\n",
    "\n",
    "単語の重要度を算出出来れば、それを文書の特徴量とみなし、クラスタリングなどに用いることもあります。\n",
    "\n",
    "実務では、検索エンジンの構築などに利用できます。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
